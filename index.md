---
layout: default
---

<img align="left" src="assets/images/Photo.jpeg" style="padding-right: 30px; padding-bottom: 30px;">
&nbsp;  
My name is Alex and I'm a researcher interested applications of machine learning to software engineering and program synthesis. Most of my work to-date has focused on AI for program optimization and leveraging tools from program analysis, compilers, and architecture research. 

I'm currently a second year PhD student at the University of Pennsylvania where I'm advised by [Osbert Bastani](https://obastani.github.io/). Before that, I spent a year working at MIT's Computer Science and Artificial Intelligence Laboratory (CSAIL) with [Yoon Kim (MIT CSAIL)](https://people.csail.mit.edu/yoonkim/) and [Jie Chen (MIT-IBM Watson AI Lab)](https://jiechenjiechen.github.io/). Before that, I was a Master's student at at Carnegie Mellon University's (CMU) School of Computer Science in the Artificial Intelligence and Innovation program where I was also a member of [Neulab](https://www.cs.cmu.edu/~neulab/index.html) where I was advised by [Professor Graham Neubig](http://www.phontron.com/). 


&nbsp;  
&nbsp;  
&nbsp;  

# \~/Publications

Shypula A<sup>*</sup>, Madaan A<sup>*</sup>, Zeng Y, Alon U, Gardner J, Hashemi M, Neubig G, Ranganathan P, Bastani O, Yazdanbakhsh A. ["Learning Performance-Improving Code Edits."](https://openreview.net/forum?id=ix7rLVHXyY). Under Review, ICLR 2024. 

Shypula A, Yin P, Lacomis J, Goues CL, Schwartz E, Neubig G. ["Learning to Superoptimize Real-world Programs."](https://arxiv.org/abs/2109.13498). Best Paper, ICLR 2022 [Deep Learning for Code Workshop](https://dl4c.github.io/).


<sup>*</sup>Equal contribution

# \~/CV and Contact

<!-- If you would like the most recent copy of my CV, you can locate it [here](assets/cv_shypula_2021_phd.pdf).  -->

You can reach me at shypula üë®‚Äçüíª seas ‚òï upenn üö¥ edu. 

# \~/Background 

I've been interested in improving the performance of deep learning models for code since my first systems course I took. AI for programming is interesting to me because of the unique feedback loops available as well as the ability to analyze and execute code that is simply not present in natural language. Because of these things, I hope that research focuses not only on how to imitate human programmers, but on how AI models can teach us ways to program and reason better like AlphaGo in [move 37](https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol#Game_2). 

I've been inspired by work on [program superoptimization](https://arxiv.org/pdf/1211.0557.pdf), [program analysis](https://cmu-program-analysis.github.io/2023/index.html), and all the exciting and cool work on RL over the years. With regard to exciting directions related to program synthesis, I love [this paper](https://arxiv.org/abs/2207.14502) for pointing out the potential for progress in this direction. Some general problems I think are important to work on now should be categorized into two bucksts: Progress (better AI programming assistants, writing unit tests, writing integration tests, software testing, program optimization, understanding generated code, debugging programs, etc.) and Safety (reducing the risks of LLMs used by malign actors, reducing the risks of producing unsafe code, etc.). 

<!-- The spark for me came when I was writing the malloc systems code for CMU's intro to system's course. It was highly error-prone, it required careful programming skill, and its performance relied on increasingly complex use of data structures from implicit lists, to linked lists, to binned linked-lists. Nevertheless, we had a large test suite that would estimate the performance of our storage allocator and provide feedback as if it were an RL environment. I wondered, "could an artificially intelligent agent learn to write code like this from scratch," re-discover classic data structures, and teach us something new? I thought it was compelling, becuase the agent would have to write programs to explain its choices, which would be more interpretable than, say, a black-box neural network.  -->

<!-- Programming is an extremely difficult and error prone task that takes many years of training and education to master: we need to create tools to lower the barriers-to-entry and democratize this amazing technology. I'm interested in how we can create machines that can write programs and design computer systems in ways that help engineers in their day-to-day work. They should help humans in their day-to-day programming tasks and every now, and then, teach us something new.  -->

<!-- Writing programs is a hard task for an AI system, because the discrete search space for programs is huge and sparse (due to the brittleness of syntax and sensitivity of semantics).  -->

<!-- I think AI on programming languages is interesting in relation to other domains like natural languages, because we can execute code and use static analysis tools on code. So, in relation to some of these other domains, we have rich representations and feedback mechanisms that we can leverage. Moreover, there may exist many tools developed by Programming Languages, Compilers, and Software Engineering community that may also both beneficial to designing these AI coding systems, possibly in ways that have not yet been conceived yet. Lastly, programs and logic are effective at computing certain kinds of things and are more interpretable than black box neural networks: they have interesting promise as components in interpretable and robust AI systems and possibly in tasks that require complex / multi-hop reasoning.  -->

<!-- On the path to get there, there are lots of super interesting and useful problems to explore in neural program synthesis / neural program modeling and software engineering.  -->

<!-- Before my research career, I majored in Business at NYU's Stern School of Business and studied Chinese. I loved learning Chinese, because for some reason as a kid, I thought it required some kind of superintelligence to learn: it taught me that discipline, a love of learning, and time are truly the ingredients to master anything.  -->


<!-- It was when I worked at IBM and saw the potential of deep learning to change our daily lives: from how we write programs, solve problems, and run drive-thrus, that I made the leap to devote my life to AI research and its applications. It has taken a lot of work, but I haven't looked back once.  -->

# \~/Other

I am currently really into specialty coffee, and I'm always looking to connect with people on this topic as well; hit me up if you want to chat about coffee! I've recently even begun to roast coffee to learn more about the process. 

<!-- 
Besides CS research, I like walking up big, tall, snowy things: some mountains on my list are (in increasing altidude) [Mt. Rainier](https://en.wikipedia.org/wiki/Mount_Rainier), [Denali](https://en.wikipedia.org/wiki/Denali), [Aconcagua](https://en.wikipedia.org/wiki/Aconcagua), and [Cho Oyu](https://en.wikipedia.org/wiki/Cho_Oyu). Here are [some photos](baker.md) of me doing a (very) stochastic gradient ascent on a practice rescue line in a crevasse on Mt. Baker.  -->

